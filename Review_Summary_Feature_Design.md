# ğŸ§  Review/Summary Feature â€” Technical Design

### ğŸŒŸ **Goal**
Add a **post-labeling intelligence layer** that:
1. **Summarizes labeling sessions** (stats, time, balance, quality).  
2. **Reviews uncertain or inconsistent labels.**  
3. **Optionally uses AI (Gemini)** to generate a natural-language summary or insights.

---

## âš™ï¸ 1. Overview

**Feature Name:** `Review/Summary`  
**Main Script:** `review_summary.py`  
**Input:** JSON log from labeling session  
**Output:**  
- Console summary  
- JSON + Markdown/HTML report  
- Optional Gemini-generated text summary  

---

## ğŸ§© 2. Data Flow

```text
label_sentences.py 
     â†“
outputs/session_log.json 
     â†“
review_summary.py 
     â†“
reports/session_summary.json / .md / .html
```

---

## ğŸ§  3. Core Steps

| Step | Function | Description |
|------|-----------|-------------|
| 1ï¸âƒ£ | `load_data()` | Load JSON logs of labeled items |
| 2ï¸âƒ£ | `compute_stats()` | Count total labels, average time, label distribution |
| 3ï¸âƒ£ | `detect_conflicts()` | Find inconsistent / long-duration / low-confidence labels |
| 4ï¸âƒ£ | `generate_summary()` | Print concise stats to terminal |
| 5ï¸âƒ£ | `save_report()` | Export JSON + human-readable Markdown/HTML summary |
| 6ï¸âƒ£ *(optional)* | `ai_summary()` | Use Gemini API to generate text review summary |

---

## ğŸ’» 4. Example CLI Usage

```bash
python review_summary.py --input logs/session_2025-10-29.json
```

**With AI Summary:**
```bash
python review_summary.py --input logs/session_2025-10-29.json --ai
```

---

## ğŸ§® 5. Key Functions (Pseudocode)

```python
def compute_stats(data):
    total = len(data)
    avg_time = sum(item["duration"] for item in data) / total
    label_counts = Counter(item["label"] for item in data)
    return {"total": total, "avg_time": avg_time, "label_counts": label_counts}

def ai_summary(stats):
    from google import generativeai as genai
    import os
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    prompt = f"Summarize labeling session insights:\n{stats}"
    model = genai.GenerativeModel("gemini-1.5-pro")
    return model.generate_content(prompt).text
```

---

## ğŸ“Š 6. Example Output (Console)

```
ğŸ§¾ Review Summary â€” session_2025-10-29
--------------------------------------
Total pairs labeled: 320
Average time per label: 5.2 s
Label balance: Positive 62% | Negative 38%
Disagreements detected: 4
--------------------------------------
AI Insight: "You labeled efficiently, but some negatives show high hesitation â€” consider clarifying class definitions."
```

---

## ğŸ§± 7. Folder Structure

```
human-in-the-loop-data-labeler/
â”‚
â”œâ”€â”€ label_sentences.py
â”œâ”€â”€ review_summary.py      # <-- new feature
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ stats.py
â”‚   â”œâ”€â”€ visualize.py
â”‚   â””â”€â”€ ai_summary.py
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ session_2025-10-29.json
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ summary_2025-10-29.md
â””â”€â”€ README.md
```

---

## ğŸš€ 8. Expansion Ideas

| Future Upgrade | Description |
|----------------|-------------|
| ğŸ“ˆ **Charts (optional)** | Add simple bar chart or pie chart via `matplotlib` |
| ğŸ” **Interactive Review** | Filter and re-label flagged items |
| ğŸ¤ **Multi-Labeler Comparison** | Add inter-annotator agreement metrics |
| ğŸ“¦ **Export Skill (MCP)** | Wrap report generation as a Skill or MCP endpoint for AI systems |

---

## ğŸ’¬ 9. AI-Vibecoding Philosophy

Keep it **simple, modular, and observable**:
- *Human-readable â†’ Machine-actionable.*
- *Every function can stand alone as a â€œSkill.â€*
- *Outputs are explainable by both humans and AI collaborators.*
